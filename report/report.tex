\documentclass[11pt,a4paper]{article}

\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{cite}
\usepackage{float}
\usepackage{multirow}
\usepackage[]{algorithm2e}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\usepackage{todonotes}                %% notes from the authors

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\newcommand\Tstrut{\rule{0pt}{2.6ex}}       % "top" strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts
\newcommand*{\escape}[1]{\texttt{\textbackslash#1}}

\fancypagestyle{firstpagefooter} {
	\lfoot{\tiny{Version: 25.09.2018}}
	\cfoot{}
	\rfoot{\thepage}
	
}

\lfoot{Name: Nicolas K\"uchler Legi: 14-712-129}
\rfoot{\thepage}

\begin{document}

\title{Advanced Systems Lab Report\\ \normalsize{Autumn Semester 2018}}
\author{Name: Nicolas K\"uchler\\Legi: 14-712-129}
\date{
	\vspace{4cm}
	\textbf{Grading} \\
	\vspace{0.5cm}
	\begin{tabular}{|c|c|}
		\hline  \textbf{Section} & \textbf{Points} \\
		\hline  1                &                 \\ 
		\hline  2                &                 \\ 
		\hline  3                &                 \\ 
		\hline  4                &                 \\ 
		\hline  5                &                 \\ 
		\hline  6                &                 \\ 
		\hline  7                &                 \\ 
		\hline \hline Total      &                 \\
		\hline 
	\end{tabular} 
}
\maketitle
\thispagestyle{firstpagefooter}

\newpage

\section{System Overview (75 pts)}

The column width is: \the\columnwidth

\todo{check 20-35 pages}
%Describe the implementation of your system and highlight design decisions relevant for the experiments. Explain how messages are parsed and how statistics are gathered in a multi-threaded setting. Provide figures containing all the threads and queues in your system (including the network and the memcached servers). Include illustrations that show how requests of different types are handled (e.g., components involved in processing the request and method calls). Please include all details necessary to understand artifacts and effects in your experiments that arise from your implementation choices. \todo{remove}


The system is a middleware platform for the popular main-memory key-value store \emph{memcached}. \todo{cite memcached.org}
It enables to balance the read workload between up to three memcached instances possibly located on different servers by replicating all writes to all memcached instances.


\subsection{Middleware Architecture}
\subsubsection{High-Level Overview}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{data/system-overview.png}
	\caption{System Overview}
\end{figure}
\todo{do properly with pdf and number components to reference them}
The middleware system is composed of three main components. 
A net-thread which uses a Java Nio Selector to handle multiple socket channels to clients listens on a tcp port for client requests. and decodes the incoming requests and puts them into a queue.

A request queue (\emph{LinkedBlockingQueue.java}) that buffers decoded requests while they are waiting for a worker-thread to process them.

Multiple worker-threads that take requests from the queue and process them according to their type.

\paragraph{Start}
Use command line  arguments to configure the middleware.
On start up of the middleware a configured number of worker-threads and a single net-thread are started.
Each worker-thread builds a tcp connection to each memcached server which is kept open during the run of the middleware.

\paragraph{Run}
The net-thread \emph{(NetThread.java)} listens on a configured tcp port for client requests. The different client channels are handled using a Selector.
After decoding the incoming request in the net-thread (see \ref{request-decoding}) they are put into a queue.

The worker-threads \emph{(WorkerThread.java)} take requests from the queue and process them according to their their type. (see \ref{request-processing}). Then they send back the response to the client via the channel.


\paragraph{Shutdown}
To stop the middleware a shutdown hook was registered that listens for a linux kill command.
Interrupt the net-thread and all worker-threads and ensures all logs have been written to the respective log file.

\subsubsection{Request}
The middleware supports set, get and multi-get requests.

Logically each request from a client is represented using an instance of \emph{SetRequest.java}, \emph{GetRequest.java} or \emph{MultiGetRequest.java}. They all inherit basic functionality from \emph{AbstractRequest.java}.
These request objects are created in the net-thread by the decoder (see \ref{request-decoding}) and put into a \emph{BlockingQueue} where they wait to be processed by a worker-thread (see \ref{request-processing}).

Apart from containing the request command, they also serve as a container for the tcp socket channel to the client and for timestamps collected at different points in the system (see \ref{measured-points}).

In addition they also contain information about their assigned memcached server id.

\subsubsection{Request Decoding}\label{request-decoding}
In the net-thread the incoming requests are decoded one by one using the function \emph{decode(ByteBuffer buffer)} of the request decoder \emph{(RequestDecoder.java)}.
Since every request needs to pass this function the decoding is done on byte level to avoid unnecessary overhead.
Apart from checking that the client request is supported by the middleware the decoder also verifies that the request is completely read. In case the decoder encounters an unsupported operation or a malformed request \emph{(UnknownRequest.java)}, the request is discarded until a newline character is read. Then an error is returned to the client and a log entry is created. If a request is not complete yet, the decoder signals the net-thread that more is expected and the net-thread will append the next content arriving from the client channel to the same buffer to complete the request.

In the following the decoding of the three different request types is explained. \todo{maybe add decoding state machine}

\paragraph{SET} \texttt{set <key> <flags> <exptime> <bytes>[noreply]\textbackslash r\textbackslash n<datablock>\textbackslash r\textbackslash n} 

After reading the set in the beginning of the command, the decoder skips over key, flags and expiration time before reading the bytes field to determine if the data block is complete. If the request is complete, a \emph{SetRequest.java} is created.

\paragraph{GET / MULTI-GET} \texttt{get <key>\textbackslash r\textbackslash n} or \texttt{get <key1> <key2> ... <key10>\textbackslash r\textbackslash n}

After reading the get in the beginning of the command, the decoder determines the number of keys and checks that the request is complete. If there is only a single key, then a \emph{GetRequest.java} is created. Otherwise a \emph{MultiGetRequest.java} is created.

For get and multi-get requests the decoder assigns each request a server (or for sharded mutli-get multiple servers) in a round-robin scheme. (see more on workload balancing in \ref{workload-balancing})


\subsubsection{Request Processing}\label{request-processing}

A worker-thread takes a request from the queue and processes it depending on the request type.

\emph{ServerMessage.java} by calling the \emph{getServerMessages()} function of the request object.

The worker-thread sends these messages one after the other to the specified servers.
The different channels to the servers are monitored using a Selector. Whenever a response of a server arrives it 
is given to the request using the function \emph{putServerResponse(serverId, buffer)}. The request assembles all responses and when all arrived a response is written back to the client through the socket channel.

\paragraph{Set} The request command is sent to all servers. If all servers answer with \texttt{STORED\textbackslash r\textbackslash n} then \texttt{STORED\textbackslash r\textbackslash n} is sent to the client. 

If at least one server answers with an error message, then the error message which arrived last is relayed to the client.

\paragraph{Get} The request command is sent to the server specified by the round-robin scheme in the net-thread.
The response of the server is directly relayed to the client.

\paragraph{MultiGet} The processing is different depending on the sharded/non-sharded mode. In case of non-sharded mode, the processing is identical to a get request where the message is sent to one server and the response is directly relayed to the client. In case of sharded mode, the request is split into up to three smaller requests\todo{explain splitting rule} . Each of them is sent to the server determined by the round-robin scheme. Then each server response is fed back to the request that is responsible for reordering the results. When all responses arrived, the reordered response is sent to the client.

\subsubsection{Workload Balancing}\label{workload-balancing}
The round robin scheme applied for get and multi-get requests allows to balance the read-workload among multiple servers.

As described in \ref{request-decoding} each get and multi-get request becomes the server ids allocated in the net-thread.
For get and multi-get requests in non-sharded mode this is only one server id. For multi-get requests in sharded mode this is a set of server ids. (if the number of keys is larger than the number of server then this will be all servers).

Since the net-thread is a singleton and the request queue offers first come first served processing of requests, the order in which they are processed does not change and hence when neglecting network related effects it can be expected that the work will be distributed evenly among the available servers.

Another form of work balancing takes place with the different number of worker threads. For a single worker-thread there is no balancing. \todo{continue this argument}

The balancing of the read workload comes at the cost of the write-workload. Since the value in each set request is replicated in all memcached servers, the server service time is determined by the slowest server. 


\subsubsection{Logging Infrastructure}

log4j2 asynchronous loggers

\todo{cite: https://logging.apache.org/log4j/2.x/manual/async.html}

\subsection{Experimental Setup}

\subsubsection{Metrics}\label{measured-points}



\begin{tabular}{|c|l|l|}
	\hline 
	\textbf{Origin} & \textbf{Metric} & \textbf{Description} \Tstrut \\ 
	\hline 
	\multirow{2}{*}{client} & - throughput & as measured by memtier \Tstrut \\ 
	& - response time & as measured by memtier \\ 
	\hline 
	\multirow{8}{*}{mw} & - throughput & per 5 second window \Tstrut \\ 
	& - response time & time between request arrived in mw and left mw\\ 
	& - net-thread processing time & time between request arrived and was enqueued\\  
	& - queue waiting time & time request was in the queue \\ 
	& - worker-thread processing time &  \\ 
	& - server service time & time memcached needed to process the request\\  
	& - queue length & sampled every 5 seconds \\  
	& - request arrival rate & per 5 second window \\ 
	\hline 
\end{tabular} 
\todo{write explanation of worker-thread processing time}

In every graph of the report the origin of the data (i.e. middleware or client) is presented usually in the bottom right corner.


\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{data/response-time-decomposition.png}
	\caption{components of client response time}
\end{figure}
\todo{do properly with pdf and color scheme}

\subsubsection{Statistic}

Each worker-thread has a separate statistic unit \emph{(Statistic.java)} which is updated after the processing of each request with the method \emph{update(request)}.  
The statistic unit is collecting online averages and sample standard deviations of the metrics over a five second window using \emph{Welford's Algorithm}.\cite{Knuth:1997:ACP:270146}

\todo{mention record count, M2 and avg => can calculate sample variance}

\begin{equation}
	\bar{x}_n = \bar{x}_{n-1} + \frac{x_n -\bar{x}_{n-1}}{n}
\end{equation}

\begin{equation}
	M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n)
\end{equation}

\begin{equation}
	s^2_n = \frac{M_{2,n}}{n-1}
\end{equation}

Essentially this allows to get an insight into the respective metric without affecting the system performance too much through extensive logging of the performance of every single request to a file. 

Every five seconds the metrics are logged to a file and then reset afterwards.

This allows to calculate offline three different error every metrics for each 

- standard deviation within the 5 second window
- standard deviation of 5 second averages over the run of the middleware
- standard deviation over experiment repetitions 



\subsubsection{Base Configuration}
- memcached (version) 1 thread
- 4096B values
- memtier benchmark (version) with configuration
\subsubsection{Simulation}\label{simulation}

The simulation results were obtained using up to 8 virtual machines on the \emph{Microsoft Azure} cloud:
\begin{itemize}
	\item 3 Client VMs of type Basic A2 (2 vcpus, 3.5 GB memory) each running one or two instances of the \emph{memtier benchmark}
	\item 2 Middleware of type Basic A4 (8 vcpus, 14 GB memory) running the middleware software
	\item 3 Server VM of type Basic A1 (1 vcpus, 1.75 GB memory) each running an instance of the \emph{memcached} key value store
\end{itemize}

All experiments were orchestrated with the \emph{python} script outlined in algorithm \ref{exp-suite-algo} running on a local machine that uses temporary \emph{ssh} connections to start, initialize and stop the run of an experiment. After each run  the resulting log files were transferred to the local machine and stored in a \emph{MongoDB}.
 
Before every experiment the network topology of all involved VMs was analysed by running a bandwidth test using \emph{iperf}. This in combination with the 4096B value size results in an estimate for the maximal achievable throughput for the given VM configuration and helps to identify when the network bandwidth is the bottleneck of the system.

The obtained results are filtered (10 seconds warmup and cooldown phase) and aggregated off-line using \emph{MongoDB} queries before being visualized using matlibplot.

\begin{algorithm}
	\ForEach
	{
		set of experiments
	}{
		- measure bandwidth and ping of the current network topology using \emph{iperf} and \emph{ping}
		
		\ForEach{experiment}{
			\ForEach
			{
				repetition
			}{
				- start all memcached instances with one thread
				
				- initialize all memcached instances by populating the key value store to avoid cache misses
				
				- start all middlewares according to the config
				
				- start all client benchmarks for 80 seconds with a value size of 4096B according to the config
				
				- wait 80 seconds
				
				- stop all middlewares
				
				- stop all memcached instances
				
				- transfer results using \emph{SCP}
				
				- store results in local \emph{MongoDB}
			
				- validate results
			}
		}
	}
	\caption{Each section of the report represents a set of experiment where different configurations were evaluated using at least three repetitions each.}\label{exp-suite-algo}
\end{algorithm}


- baseline without middleware run separately all other experiments were run without shutting down the vm to allow comparison of results not only within an experiment but also between experiments.
- after 10 sec warm up and cooldown phase ensured stable 1 minute throughput performance by checking coefficient of variation. In case an external source on the cloud prevented a stable execution phase of one minute, the experiment was repeated.
- before every run, restart all memcached instances, initialize them by filling all keys, restart mw's



\todo{System Overview
(Mapping code to functionality / Explanation how queues and threads are implemented 15)
insert overview picture}

- Description of the data-structures for holding connections 6 

- Parsing Requests: 
(Description how requests are parsed 6)
\todo{insert processing schematic}

- Processing of Requests in Worker:
- Set Request
- Get Request
- MultiGet Request (Sharded vs Non Sharded)
Description how SET requests are processed 6 Description how GET requests are processed 6 Description how Multi-GET requests are processed 6 

- Description how work is balanced (if not round-robin: proof required) 15 
-> here get, multiget sharded, maybe also worker balances work

- Statistics (Explanations related to statistics 15)
-> Sampling of queue size, arrival rate in 5 sec windows
-> Welfords Algorithm Online Averages with Std Deviation within 5 sec windows, reported 5sec aggregations (allows insight std dev gives insight in window without slowing down system too much through logging)
\todo{cite welford algorithm}


\section{Baseline without Middleware (75 pts)}\label{exp2}

In this section the performance characteristics of the memtier clients and memcached servers in the azure cloud are studied.

\subsection{One Server}\label{exp21}

%Both, for a read-only and write-only workload plot the throughput and the response time as a function of NumClients. All clients are connected to a single memcached instance.

%Use 3 load generating VMs, with one memtier (CT=2) each, and vary the number of virtual clients (VC) per memtier thread between 1 and 32. Show how the behavior of the server changes as we add more clients.

This experiment analyses the behaviour of the system with 3 load generating VM's and a single server VM. The detailed experiment configuration is presented in the table below.

For every configuration the throughput and response time as measured on the client is used.

Every graph contains the sample standard deviation over the 3 repetitions as an error metric.
To validate the measurements the interactive law with a client thinking time Z of 0 is shown in every throughput and response time graph.

As mentioned in section \ref{simulation} before running the experiment the network bandwidth between the VM's was tested using \emph{iperf}. From these measurements a maximal achievable throughput was derived by considering the value size of 4096B. This bandwidth limit is also shown in the throughput graphs.


\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 1                        \\ 
			\hline Number of client machines        & 3                        \\ 
			\hline Instances of memtier per machine & 1                        \\ 
			\hline Threads per memtier instance     & 2                        \\
			\hline Virtual clients per thread       & [1, 2, 4, 8, 12, 16, 24, 32]\\ 
			\hline Workload                         & Write-only and Read-only \\
			%\hline Multi-Get behavior               & N/A                      \\
			%\hline Multi-Get size                   & N/A                      \\
			%\hline Number of middlewares            & N/A                      \\
			%\hline Worker threads per middleware    & N/A                      \\
			\hline Repetitions                      & 3 (at least 1 minute each)\\ 
			\hline 
		\end{tabular}
	}
\end{center}


\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp21_ro_tp_nc.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp21_ro_rt_nc.pdf}
	\end{subfigure}%
	\caption{Throughput and response time with interactive law in read-only workload with one memcached server.}
	\label{exp21_ro_nc}
\end{figure}


\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp21_wo_tp_nc.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp21_wo_rt_nc.pdf}
	\end{subfigure}%
	\caption{Throughput and response time with interactive law in write-only workload with one memcached server.}
	\label{exp21_wo_nc}
\end{figure}



\subsubsection{Explanation}

%Describe in which phase the memcached servers are under-saturated, saturated, or over-saturated. Describe how throughput and response time correlate. Explain what further conclusions can be drawn from the experiment.

As shown in figure \ref{exp21_ro_nc}, the throughput saturation for the read-only workload is already reached at 6 clients because the upload bandwidth of the single server VM limits the throughput. Before the experiment, the server VM upload bandwidth was measured to be 99.6 MBit/sec. When using a value size of 4096B, this results in a maximum possible throughput of 3040 ops/sec for a read-only workload because all the values need to be sent from the server VM to one of the 3 client VM's. 

Consequently increasing the number of clients has almost no effect on the throughput while the response time grows linearly because more clients results in more requests on the server that wait to be sent to a client VM. \todo{maybe be more precice about why linear increase: I think more clients => more requests => every request needs to "wait" a bit longer on server vm}

In figure \ref{exp21_wo_nc} it can be observed that for a write-only workload the throughput saturation is reached at 72 clients. In this experiment the bottleneck is not the bandwidth but rather the memcached server. 
As expected the throughput first increases in the number of clients while the memcached servers are still under-saturated and then the response 


\subsection{Two Servers}\label{exp22}

%For a read-only and write-only workload plot throughput and response time as a function of NumClients. The clients are connected to two memcached instances. 

%Use 1 load generating VM, with one memtier (CT=1) connected to each memcached instance (two memcache instances in total), and vary the number of virtual clients (VC) per memtier thread between 1 and 32. Show how the behavior of the server changes and explain what conclusions we can draw from this experiment.

\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 2                        \\ 
			\hline Number of client machines        & 1                        \\ 
			\hline Instances of memtier per machine & 2                        \\ 
			\hline Threads per memtier instance     & 1                        \\
			\hline Virtual clients per thread       & [1, 2, 4, 8, 12, 16, 24, 32]\\ 
			\hline Workload                         & Write-only and Read-only \\
			%\hline Multi-Get behavior               & N/A                      \\
			%\hline Multi-Get size                   & N/A                      \\
			%\hline Number of middlewares            & N/A                      \\
			%\hline Worker threads per middleware    & N/A                      \\
			\hline Repetitions                      & 3 (at least 1 minute each) \\ 
			\hline 
		\end{tabular}
	} 
\end{center}



\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp22_ro_tp_nc.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp22_ro_rt_nc.pdf}
	\end{subfigure}%
	\caption{Throughput and response time with interactive law in read-only workload with one load generating VM.}
	\label{exp22_ro_nc}
\end{figure}




\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp22_wo_tp_nc.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp22_wo_rt_nc.pdf}
	\end{subfigure}%
	\caption{Throughput and response time with interactive law in write-only workload with one load generating VM.}
	\label{exp22_wo_nc}
\end{figure}



\subsubsection{Explanation}

%Describe how this experiment compares to the previous section. Which results are the same and which ones differ? Explain what further conclusions can be drawn from the experiment.

For a read-only workload the throughput saturation is reached at 8 clients, as shown in figure \ref{exp22_ro_nc}, because the upload bandwidth of the two servers limits the possible throughput. The measured total upload bandwidth between the 2 server VM's and the client VM is 199 MBit/sec which results in an estimate of 6062 ops/sec for the maximally achievable throughput when using values of 4096B. Before the saturation point is reached at 8 clients, increasing the number of clients results in a higher throughput as expected because the system is still under-saturated. \todo{maybe mention that because cutoff is so abruptly that it could be expected that with 8 clients memcached itself is not saturated at all}

Using a write-only workload with values of 4096B the situation is similar but here instead of the upload bandwidth of the server VM's the bottleneck is the upload bandwidth of the load generating client VM. The measured 199 MBit/sec total upload bandwidth of the client VM limits the throughput of the system to 6062 ops/sec. So as a consequence  the throughput saturation is reached at \todo{choose either 8 or 16} clients as shown in figure \ref{exp22_wo_nc}.


\subsection{Summary}


\begin{center}
	{Maximum throughput of different VMs.}
	\begin{tabular}{|l|p{2cm}|p{2cm}|p{4cm}|}
		\hline                        & Read-only workload & Write-only workload & Configuration gives max. throughput \\ 
		\hline One memcached server   & 2962 ops/sec       & 14097 ops/sec & 6 clients for read-only 72 clients for write-only\\ 
		\hline One load generating VM & 5878 ops/sec       &  5802 or 6012 ops/sec & 8 clients for read-only 8/16 clients for write-only\\ 
		\hline 
	\end{tabular}
\end{center}


%Write at least two paragraphs about how both results relate. Describe what is the bottleneck of this setup is. If the maximum throughput for both experiments is the same, explain why. If it is not the case, explain why not. Write down key take-away messages about the behaviour of the memtier clients and the memcached servers.

\paragraph{Comparison of one memcached server vs. one load generating VM}
For a read-only workload both experiments are bound by the network upload bandwidth of the server VM(s).
Despite the fact that they are both network bound, they have a different maximum throughput because having an additional server in the second experiment gives two times the upload bandwidth between server and client VMs and thus resulting in approximately two times the throughput.

The different throughput for write-only workload is explained by considering that only the second experiment is bound by the network. Namely by the upload bandwidth of the client VM. The bottleneck in the first experiment is memcached which saturates for around 72 clients.

\paragraph{Comparison of workloads}
In the first experiment with one memcached server the throughput is different between write and read workload. This is because in a read-only workload the server needs to transport the value and some control structure which is more than 4096B. For write-only the server VM only needs to transmit the 8 byte confirmation message \texttt{STORED\textbackslash r\textbackslash n} to the client VM. 

Despite the fact that in the second experiment with only one load generating VM the throughput for read-only and write-only is almost identical the bottleneck is a different part of the system. For read-only it is the upload bandwidth of the two server VMs and for the write-only workload it is the upload bandwidth of the load generating client VM.

\paragraph{Key take-away messages}
\begin{itemize}
	\item a single server VM cannot handle more than 3000 ops/sec in a read-only workload
	\item a single client VM cannot simulate a higher write-only workload than 6000 ops/sec
	\item in a write-only workload the point of saturation for a single memcached server is around 14000 ops/sec
\end{itemize}


\section{Baseline with Middleware (90 pts)}\label{exp3}

%In this set of experiments, you will have to use 1 load generator VM and 1 memcached server, measuring how the throughput of the system changes when increasing the number of clients. Scaling virtual clients inside memtier has to be done as explained in the previous sections. Plot both throughput and response time as measured on the middleware.

In this set of experiments the impact of the number of clients on the  performance of the system is studied when using 3 load generating VMs and 1 memcached server. 

All measurements are validated using the interactive law, which describes the relationship between throughput $X$ and response time $R$ in a closed system with $N$ clients where each client has a client waiting time of $Z$ (Eq. \ref{ilaw}). For all experiments a client waiting time of $Z = 0$ is used.

\begin{equation}\label{ilaw}
R = \frac{N}{X} - Z
\qquad\qquad\qquad\qquad
Error = R_{client} - (\frac{N}{X_{mw}}  - Z)
\end{equation}

However the interactive law cannot be applied directly to validate the throughput and response time measurements from the middleware.
This is due to the fact that the middleware cannot measure the time a request needs for the transport over the network between client and middleware. This results in a shorter response time measurement in the middleware than the actual response time measured on the client.
Hence when using the middleware response time, this would result in a throughput that is higher than the measured throughput.
To circumvent this issue while still being able to use the interactive law as a sanity check for the collected data, the response time as measured on the client is used instead. In addition a manual check was applied that the response time differences between client and middleware measurements is approximately the round trip time between client VM and middleware VM: $R_{client} - R_{mw} \approx rtt$.


\subsection{One Middleware}\label{exp31}

%Connect one load generator machine (one instance of memtier with CT=2) to a single middleware and use 1 memcached server. Run a read-only and a write-only workload with increasing number of clients (between 2 and 64) and measure response time \emph{both at the client and at the middleware}, and plot the throughput and response time measured in the middleware.

%Repeat this experiment for different number of worker threads inside the middleware: 8, 16, 32, 64.
Three load generating VMs are connected to one middleware handling requests for a single server. 
The number of clients is varied between 6 and 288 depending on the saturation of the system with 8, 16, 32 and 64 worker threads inside the middleware for both a read-only and write-only workload. The details of the configuration are shown in the table below.

\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 1                        \\ 
			\hline Number of client machines        & 3                        \\ 
			\hline Instances of memtier per machine & 1                        \\ 
			\hline Threads per memtier instance     & 2                        \\
			\hline Virtual clients per thread       & [1, 2, 4, 8, 12, 16, 24, 32, 48] \\ 
			\hline Workload                         & Write-only and Read-only \\
			%\hline Multi-Get behavior               & N/A                      \\
			%\hline Multi-Get size                   & N/A                      \\
			\hline Number of middlewares            & 1                        \\
			\hline Worker threads per middleware    & [8, 16, 32, 64]                  \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)                \\ 
			\hline 
		\end{tabular}
	} 
\end{center}


The minor deviations from the interactive law in the table \ref{exp31_ilaw} result from small differences in the measured average throughput on client and middleware. This can be explained because the warmup and cooldown phase are excluded in the middleware measurements while they are present in the measurements from \emph{memtier benchmark}. As the number of clients increases the deviations also increase but when putting that into relation with the response time measurements they remain marginal. This confirms that the interactive law holds for all measurements in this section.


\begin{table}
	\scriptsize{
		\centering
		\setlength{\tabcolsep}{4.5pt}
		\begin{tabular}{|cr|*{9}{r}|*{7}{r}|}
			\cline{3-18}
			\multicolumn{2}{c|}{} & \multicolumn{9}{c|}{number of clients} & \multicolumn{7}{c|}{number of clients} \Tstrut\\
			\multicolumn{2}{c|}{} & 6 & 12 & 24 & 48 & 72 & 96 & 144 & 192 & 288 & 6 & 12 & 24 & 48 & 72 & 96 & 144 \\
			\hline
			\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{worker}}} & 8 & 0.0 & -0.1 & 0.0 & 0.1 & 0.3 & 0.6 & - & - & - & 0.0 & 0.1 & 0.3 & 0.4 & 1.3 & 2.0 & 1.8\Tstrut\\
			& 16 & -0.1 & 0.0 & 0.0 & 0.1 & 0.1 & 0.2 & 0.0 & 0.4 & - & 0.0 & 0.0 & 0.1 & 0.9 & 1.2 & 1.0 & 1.1 \\
			& 32 & -0.1 & -0.1 & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.2 & - & 0.0 & 0.0 & 0.2 & 0.6 & 0.5 & 0.7 & 1.1 \\
			& 64 & 0.0 & 0.0 & 0.0 & 0.0 & -0.2 & 0.0 & 0.1 & 0.2 & 0.9 & 0.0 & 0.0 & 0.1 & 0.5 & 0.5 & 0.7 & 1.1 \\
			& & \multicolumn{9}{c|}{in milliseconds} & \multicolumn{7}{c|}{in milliseconds}\\
			\hline
			\multicolumn{2}{c}{} & \multicolumn{9}{c}{write-only} & \multicolumn{7}{c}{read-only} \Tstrut\\ 
		\end{tabular}
		\caption{Interactive law response time deviations in milliseconds from client measurements in setting with one middleware according to equation \ref{ilaw}.}\label{exp31_ilaw}
	}
\end{table}


\subsubsection{Explanation}

\paragraph{Read-Only Workload}

\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_ro_tp_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_ro_rt_nc_w.pdf}
	\end{subfigure}%
	\caption{Throughput and response time in read-only workload for the system with a single middleware for different number of clients. The sample standard deviation over the experiment repetitions is used as the error metric in both graphs.}\label{exp31_ro_tp_nc}
\end{figure}

As described in section \ref{exp2} the read-only workload with a single \emph{memcached} server VM and values of size of 4096B is bound to around 3000 ops/sec by the upload bandwidth of the server VM. This phenomena is also clearly visible in figure \ref{exp31_ro_tp_nc} when using a single middleware in between client VMs and server VM. Consequently the point of throughput saturation of a read-only workload is reached already with 6 clients. 

In the following it is analysed how this network bandwidth bottleneck manifests itself in the middleware measurements.

As long as the number of clients is less than the number of workers in the middleware the request queue in the middleware is empty (Fig. \ref{exp31_ro_q}). This is because the number of clients in a closed system put a limit on the number of requests that can be concurrently in the system. 
So when there are more worker-threads than possible requests in the system, every request placed in the queue is processed immediately by one of the idle workers. This is a fact that holds independently of the network bottleneck. 

Figure \ref{exp31_ro_tp_nc} shows that the response time is not affected by the number of worker-threads because already using 8 workers is enough in combination with 12 clients to reach the network bottleneck so adding more workers cannot improve the throughput. 
However as the number of clients are increased the response time also increases because as explained above the requests that cannot be sent through the network bottleneck are queued on the server or when all workers are already waiting for a response from a server then queued in the middleware.

An interesting observation is made when looking at the different components of the response time.
The queue waiting time and the server service time are the two dominant factors of the response time as measured on the client while the network-, the net-thread decoding- and worker-thread processing time are negligibly small  (Fig. \ref{exp31_ro_rtcomp}). As the number of workers increases, requests spend less time in the queue and instead stay longer on the server VM where they are waiting in front of the network bottleneck. 
So the number of workers does not influence the response time but it influences if the requests are queued in the middleware or on the server VM.

The server service time increases up to the point where are all workers are busy and the queue starts to fill up and afterwards it remains constant because the number of workers in the middleware limit the number of requests that can be concurrently at the server VM and hence need to be sent through the network bottleneck. (Fig. \ref{exp31_ro_sst})


%Provide a detailed analysis of the results (e.g., bottleneck analysis, component utilizations, average queue lengths, system saturation). Add any additional figures and experiments that help you illustrate your point and support your claims.


\begin{figure}
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_ro_queue_nc_w.pdf}
		\caption{}\label{exp31_ro_q}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_ro_sst_nc_w.pdf}
		\caption{}\label{exp31_ro_sst}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_ro_rt_component_w.pdf}
		\caption{}\label{exp31_ro_rtcomp}
	\end{subfigure}
	\caption{Queue size and server service time with varying number of clients and client response-time component analysis for 96 clients with different number of worker-threads in a read-only workload with one middleware.}
\end{figure}


\paragraph{Write-Only Workload}


The data in figure \ref{exp31_wo_tp_nc} indicates that the throughput saturates for different number of clients when varying the number of worker-threads. This is because there is a trade-off between server service time and queueing time in the middleware when using a different number of workers. For fewer worker, requests start to queue up already with a smaller number of clients because the number of worker-threads controls how many requests can be sent to the server concurrently. However with more requests at the server concurrently, the server also has a longer server service time for each of them.

The throughput saturation for 8 workers is reached at \todo{x} clients, for 16 workers at \todo{x} clients, for 32 workers at \todo{x} clients and for 64 workers at \todo{x} clients.

For every number of worker-threads eventually the waiting time in the queue becomes the dominant factor in the response time when all workers are busy waiting for a response from the server.

The component utilization figure shows that the bottleneck in the system is the number of worker-threads in the middleware that are waiting concurrently for a response of the server. This suggests that increasing the number of worker-threads would further benefit the throughput. However, the increasing server service time for an individual request with a larger number of worker-threads places a limit on this throughput increase.

When there are more clients than worker-threads, then the number of requests in the queue is approximately equal to the difference between number of clients and number of worker-threads.

As in the read-only workload the net-thread decoding and worker-thread processing time remain negligible factors in the response time.




\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_tp_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_rt_nc_w.pdf}
	\end{subfigure}
	\caption{Throughput and response time in write-only workload for the system with a single middleware for different number of clients. The sample standard deviation over the experiment repetitions is used as the error metric in both graphs.}\label{exp31_wo_tp_nc}
\end{figure}


\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_queue_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_sst_nc_w.pdf}
	\end{subfigure}\hfill
	\caption{Number of requests in queue per middleware and average server service time per request in a write-only workload with one middleware. Both graphs show the sample standard deviation over the repetitions as error metric.}\label{exp31_wo_queue_sst_nc}
\end{figure}

\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_util_nc_w16.pdf}
		\caption{16 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_util_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}\hfill
	\caption{Component utilization between 6 and 192 clients in a write-only workload with one middleware.}
\end{figure}

\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_component_nc_w16.pdf}
		\caption{16 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp31_wo_component_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}\hfill
	\caption{Response time components between 6 and 192 clients in a write-only workload with one middleware.}
\end{figure}




\subsection{Two Middlewares}\label{exp32}

%Connect one load generator machine (two instances of memtier with CT=1) to two middlewares and use 1 memcached server. Run a read-only and a write-only workload with increasing number of clients (between 2 and 64) and measure response time \emph{both at the client and at the middleware}, and plot the throughput and response time as measured in the middleware.

%Repeat this experiment for different number of worker threads inside the middleware: 8, 16, 32, 64.

%If in your experiment the middleware is not the bottleneck, repeat the experiment that reaches the highest throughput but using two load generator VMs (each with 2x memtier CT=1) instead of one. Otherwise, explain how you know that the middlewares are the limiting factor in terms of throughput.


Three load generating VMs are connected to two middlewares handling requests for a single server. 
The number of clients is varied between 6 and 384 depending on the saturation of the system with 8, 16, 32 and 64 worker-threads inside the middleware for both a read-only and write-only workload. The details of the configuration are shown in the table below.


\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 1                        \\ 
			\hline Number of client machines        & 3                        \\ 
			\hline Instances of memtier per machine & 2                        \\ 
			\hline Threads per memtier instance     & 1                        \\
			\hline Virtual clients per thread       & [1, 2, 4, 8, 12, 16, 24, 32, 48, 64] \\ 
			\hline Workload                         & Write-only and Read-only \\
			%\hline Multi-Get behavior               & N/A                      \\
			%\hline Multi-Get size                   & N/A                      \\
			\hline Number of middlewares            & 2                        \\
			\hline Worker threads per middleware    & [8, 16, 32, 64]                  \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)                \\ 
			\hline 
		\end{tabular}
	} 
\end{center}

As explained in the setting with one middleware the minor deviations from the interactive law in the table \ref{exp32_ilaw} result from small differences in the measured average throughput on client and middleware.
As the number of clients increases the deviations also increase but when putting that into relation with the response time measurements they remain marginal. This confirms that the interactive law holds for all measurements in this section.

\begin{table}
	\scriptsize{
		\centering
		\setlength{\tabcolsep}{4.5pt}
		\begin{tabular}{|cr|*{10}{r}|*{7}{r}|}
			\cline{3-19}
			\multicolumn{2}{c|}{} & \multicolumn{10}{c|}{number of clients} & \multicolumn{7}{c|}{number of clients} \Tstrut\\
			\multicolumn{2}{c|}{} & 6 & 12 & 24 & 48 & 72 & 96 & 144 & 192 & 288 & 384 & 6 & 12 & 24 & 48 & 72 & 96 & 144 \\
			\hline
			\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{worker}}} & 8 & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.4 & 0.6 & 0.7 & 1.1 & - & 0.0 & 0.0 & 0.2 & 0.8 & 1.0 & 1.4 & 2.0\Tstrut\\
			& 16 & 0.0 & 0.0 & 0.0 & 0.0 & 0.1 & 0.1 & 0.3 & 0.5 & 0.9 & - & 0.0 & 0.0 & 0.3 & 0.6 & 1.1 & 1.6 & 1.9 \\
			& 32 & -0.1 & -0.1 & -0.1 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.6 & - & 0.0 & 0.1 & 0.4 & 1.0 & 1.2 & 1.6 & 2.9 \\
			& 64 & -0.1 & -0.1 & 0.0 & 0.1 & 0.1 & 0.1 & 0.3 & 0.2 & 0.4 & 0.3 & 0.0 & 0.1 & 0.3 & 0.8 & 1.1 & 1.4 & 2.5 \\
			& & \multicolumn{10}{c|}{in milliseconds} & \multicolumn{7}{c|}{in milliseconds}\\
			\hline
			\multicolumn{2}{c}{} & \multicolumn{10}{c}{write-only} & \multicolumn{7}{c}{read-only} \Tstrut\\ 
		\end{tabular}
		
		\caption{Interactive law response time deviations in milliseconds from client measurements in the setting with two middlewares according to equation \ref{ilaw}.}\label{exp32_ilaw}
	}
\end{table}


\subsubsection{Explanation}

%Provide a detailed analysis of the results (e.g., bottleneck analysis, component utilizations, average queue lengths, system saturation). Add any additional figures and experiments that help you illustrate your point and support your claims.

\paragraph{Read-Only Workload}

\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_ro_tp_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_ro_rt_nc_w.pdf}
	\end{subfigure}%
	\caption{Throughput and response time in read-only workload for the system with a two middlewares for different number of clients. The sample standard deviation over the experiment repetitions is used as the error metric in both graphs.}\label{exp32_ro_tp_nc}
\end{figure}

As seen in the setting with a single middleware the read-workload is bound by the upload bandwidth of the server VM. Since the bottleneck is not related to the middleware it is no surprise that adding an additional middleware does not change the observed behaviour and the throughput saturation is still reached at 6 clients independent of the number of worker-threads. (Fig. \ref{exp32_ro_tp_nc})

However since the two middlewares share the workload, each middleware has only half of the clients.
Thus requests are only starting to be queued up when there are two times more clients than worker-threads (Fig \ref{exp32_ro_q}).
The same holds for the server service times that start becoming constant in the number of clients when there are more than two times more clients than worker-threads but for the same reasons outlined in section \ref{exp31}. 
The trade-off between queue waiting time and server service time as the dominating components of the response time is also evident in the setting with two middlewares (Fig. \ref{exp32_ro_rtcomp}) and so the number of workers does still not influence the total response time but it influences in which part of the system the requests are queued.



\begin{figure}
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_ro_queue_nc_w.pdf}
		\caption{}\label{exp32_ro_q}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_ro_sst_nc_w.pdf}
		\caption{}\label{exp32_ro_sst}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_ro_rt_component_w.pdf}
		\caption{}\label{exp32_ro_rtcomp}
	\end{subfigure}
	\caption{Queue size and server service time with varying number of clients and client response-time component analysis for 144 clients with different number of worker-threads in a read-only workload with two middlewares.}
\end{figure}

\paragraph{Write-Only Workload}

In the write-only workload the system with two middlewares behaves in a similar way as the system with one middleware. 
As shown in figure \ref{exp32_wo_util_nc} the bottleneck remains the number of worker-threads that are waiting for a server response, while the server service time depends only on the number of busy worker-threads (Fig. \ref{exp32_wo_rtcomp_nc})


However, by adding an additional middleware the total number worker-threads in the system is multiplied by two and hence the maximal throughput achieved in the experiments is higher for the same number of worker-threads per middleware.
For 8 worker-threads per middleware the point of saturation is reached with \todo{x} clients, for 16 workers with \todo{x} clients, for 32 workers with {x} clients and for 64 workers with \todo{x} clients.  \todo{maybe reduce to 64 workers}


The collected data indicates that the total number of worker-threads in the system is the decisive factor for performance.
So essentially the system with two middlewares has a similar performance to the system with one middleware with two times as many worker-threads. (i.e. system with two middlewares with 32 workers each has approximately the same performance as the system with a single middleware with 64 worker-threads)
This results from the fact that neither the net-thread nor the middleware VM network bandwidth is the bottleneck and so by duplicating the middleware VM the only performance gain is the consequence of more worker-threads in the system.
The total number of worker-threads in the system affect the performance because the server service time depends only on the number of worker-threads that are sending requests concurrently. (Fig. \ref{exp31_wo_queue_sst_nc} and \ref{exp32_wo_queue_sst_nc})
Consequently each request needs to wait in the queue for approximately the same time and hence the total response time is the same which leads to the same throughput in systems with the same number of total worker-threads. (Fig. \ref{exp31_wo_tp_nc} and \ref{exp32_wo_tp_nc})

The system with one middleware has on average two times as many requests in the queue compared to each queue in the system with two middlewares but the total number of requests in the whole system that are waiting in a middleware queue is the same when the total number of workers is identical. (Fig. \ref{exp31_wo_queue_sst_nc} and \ref{exp32_wo_queue_sst_nc})




\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_tp_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_rt_nc_w.pdf}
	\end{subfigure}%
	\caption{Throughput and response time in write-only workload for the system with a two middlewares for different number of clients. The sample standard deviation over the experiment repetitions is used as the error metric in both graphs.}\label{exp32_wo_tp_nc}
\end{figure}
\todo{mention with 64 worker almost reach 14k throughput from baseline}



\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_queue_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_sst_nc_w.pdf}
	\end{subfigure}\hfill
	\caption{Number of requests in queue per middleware and average server service time per request in a write-only workload with two middlewares. Both graph show the sample standard deviation over the repetitions as error metric.}\label{exp32_wo_queue_sst_nc}
\end{figure}

\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_util_nc_w16.pdf}
		\caption{16 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_util_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}\hfill
	\caption{Component utilization between 6 and 288 clients in a write-only workload with two middlewares.}\label{exp32_wo_util_nc}
\end{figure}

\begin{figure}
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_component_nc_w16.pdf}
		\caption{16 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.499\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp32_wo_component_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}\hfill
	\caption{Response time components between 6 and 288 clients in a write-only workload with two middlewares.}\label{exp32_wo_rtcomp_nc}
\end{figure}



\subsection{Summary}

%Based on the experiments above, fill out the following table. For both of them use the numbers from a single experiment to fill out all lines. Miss rate represents the percentage of GET requests that return no data. Time in the queue refers to the time spent in the queue between the net-thread and the worker threads.
\todo{ask question per e-mail that its okay to use two different configurations for workloads}

\begin{center}
	{Maximum throughput for one middleware.}
	\begin{tabular}{|l|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
		\hline                                & Throughput [ops/sec] & Response time [ms] & Average time in queue [ms] & Miss rate \\ 
		\hline Reads: Measured on middleware  &                 2814 &                1.2 &                        0.1 & 0.0       \\ 
		\hline Reads: Measured on clients     &                 2780 &                2.2 &                        n/a & 0.0       \\ 
		\hline Writes: Measured on middleware &                12030 &                9.8 &                        4.6 & n/a       \\ 
		\hline Writes: Measured on clients    &                12144 &                11.8 &                       n/a & n/a       \\ 
		\hline 
	\end{tabular}
\end{center}

\begin{center}
	{Maximum throughput for two middlewares.}
	\begin{tabular}{|l|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
		\hline                                & Throughput [ops/sec] & Response time [ms] & Average time in queue [ms] & Miss rate \\ 
		\hline Reads: Measured on middleware  &                 2869 &                1.2 &                        0.1 & 0.0       \\ 
		\hline Reads: Measured on clients     &                 2827 &                2.1 &                        n/a & 0.0       \\ 
		\hline Writes: Measured on middleware &                13569 &               12.8 &                        3.5 & n/a       \\ 
		\hline Writes: Measured on clients    &                13776 &               14.0 &                        n/a & n/a       \\ 
		\hline 
	\end{tabular}
\end{center}

%Based on the data provided in these tables, write at least two paragraphs summarizing your findings about the performance of the middleware in the baseline experiments.

\paragraph{Analysis}

As shown in sections \ref{exp31} and \ref{exp32} the read-only workload is independent of the number of middlewares in the setup with one server VM and thus in the follwing results are summarized for systems with both one and two middlewares together.
The optimal number of clients for the read-only workload is somewhere in between 6 and 12 clients but the configuration with 12 clients is already strongly affected by the bandwidth bottleneck and results in a response time that is almost two times as long with only a small gain in throughput. So under the evaluated configurations the maximum throughput is achieved with 6 clients and is independent of the number of worker-threads. Thus there is no point in using more than 8 workers. 
There are no misses because before every experiment \emph{memcached} is initialized with all keys and thus misses have no influence on the response time. Since in the optimal configuration the number of worker threads is greater than the number of clients the queue basically remains empty and consequently the average time of a request in the queue is a negligible factor in the response time.

For a write-only workload the maximum throughput  in the system with a single middleware is achieved with 144 clients and 64 middleware worker-threads. In the system with two middlewares the point of throughput saturation is reached at 192 clients when using 64 worker-threads per middleware. The maximum throughput of the system involving two middlewares comes close to the maximum throughput of 14000 ops/sec recorded in the baseline without a middleware\footnote{The experiments of this section were run after restarting the VMs used in the baseline without middleware and so the measurements are not perfectly comparable but they should be close enough.} which poses an upper bound on what can be achieved for a write-only workload. It can be concluded that the overhead of using a middleware in a write-only workload with a single server is minimal.

Since in both setups there are more clients than worker-threads, requests usually spend some time waiting in the queue. However, the additionally gained throughput outweighs the longer response time incurred by this waiting.

The difference between response time measurements on the client and the middleware is between 1 and 2 milliseconds for both workloads which is consistent with the round trip time measurements between the involved VMs using \emph{ping}. The small difference in throughput measurements can be explained by the exclusion of warm up and cooldown phase in the middleware. 

\paragraph{One middleware vs. two middlewares}

For the read-only workload having one or two middlewares does not make a difference because the server VM network bottleneck is not affected by the number of middlewares in the system.

For a write-only workload the maximal throughput achieved with two middlewares is higher than with a single middleware but  this is only an indirect consequence of having an additional middleware because the maximum total number of workers evaluated in the two systems is different and this is the important parameter for the performance as outlined in section \ref{exp32}. The maximal throughput in the system with two middlewares is achieved with 64 worker-threads per middleware and thus 128 workers in total. The configuration with a single middleware and 128 worker-threads was not evaluated but the data indicates that this could produce a similar performance assuming the middleware VM does not start to get a problem with context switches or another effect slowing down individual threads.



\paragraph{Key take-away messages}
\begin{itemize}
	\item the number of worker-threads is the decisive factor for performance in a write-only workload
	\item with two middlewares and the resulting 128 worker-threads the maximal throughput of the baseline without middleware is almost reached
	\item round trip time between client and middleware VM is varying between 1 and 2 milliseconds
\end{itemize}

\section{Throughput for Writes (90 pts)}

\subsection{Full System}

%Connect three load generating VMs to two middlewares and three memchached servers. Run a write-only experiment. 
%You need to plot throughput and response time measured on the middleware as a function of number of clients. The measurements have to be performed for 8, 16, 32 and 64 worker threads inside each middleware.
Three load generating VMs are connected to a system  consisting of two middleware VMs and three server VMs running \emph{memcached}.
The number of clients is varied between 6 and 384 and the performance of a write-only workload is evaluated for configurations with 8, 16, 32 and 64 worker-threads per middleware. The details of the configuration are shown in the table below.


\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 3          \\ 
			\hline Number of client machines        & 3          \\ 
			\hline Instances of memtier per machine & 2          \\ 
			\hline Threads per memtier instance     & 1          \\
			\hline Virtual clients per thread       & [1, 2, 4, 8, 12, 16, 24, 32, 48, 64]    \\ 
			\hline Workload                         & Write-only \\
			%\hline Multi-Get behavior               & N/A        \\
			%\hline Multi-Get size                   & N/A        \\
			\hline Number of middlewares            & 2          \\
			\hline Worker threads per middleware    & [8, 16, 32, 64]    \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)  \\ 
			\hline 
		\end{tabular}
	} 
\end{center}

Table \ref{exp41_ilaw} shows that the interactive law holds for all measurements shown in this set of experiments. As previously stated the small differences in response time are a result of small throughput measurement differences between client and middleware.


\begin{table}
	\scriptsize{
		\centering
		\setlength{\tabcolsep}{4.5pt}
		\begin{tabular}{|cr|*{10}{r}|}
			\cline{3-12}
			\multicolumn{2}{c|}{} & \multicolumn{10}{c|}{number of clients} \Tstrut\\
			\multicolumn{2}{c|}{} & 6 & 12 & 24 & 48 & 72 & 96 & 144 & 192 & 288 & 384 \\
			\hline
			\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{worker}}} & 8 & -0.1 & 0.0 & 0.0 & 0.1 & 0.1 & 0.3 & 0.7 & 1.0 & 1.6 & 1.8\Tstrut\\
			& 16 & -0.1 & -0.1 & 0.0 & 0.0 & 0.1 & 0.0 & 0.4 & 0.2 & 1.0 & 2.7 \\
			& 32 & -0.1 & -0.1 & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.4 & 0.7 & 0.5 \\
			& 64 & -0.1 & -0.1 & 0.0 & 0.0 & 0.1 & 0.1 & 0.2 & 0.3 & 0.3 & 0.9 \\
			& & \multicolumn{10}{c|}{in milliseconds} \\
			\hline
			\multicolumn{2}{c}{} & \multicolumn{10}{c}{write-only} \Tstrut\\ 
		\end{tabular}
		\caption{Interactive law response time deviations in milliseconds from client measurements in the full system according to equation \ref{ilaw}.}\label{exp41_ilaw}
	}
\end{table}



\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_wo_tp_nc_w.pdf}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_wo_rt_nc_w.pdf}
	\end{subfigure}%
	\caption{Throughput and response time in a write-only workload in the full system with different number of worker-threads per middleware as a function of the number of clients. Both graphs show the sample standard deviation of the measurements over the repetitions as an error metric.}\label{exp41_tp_rt_nc}
\end{figure}


\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_util_nc_w8.pdf}
		\caption{8 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_util_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}%
	\caption{Component utilization}\label{exp41_util_nc}
\end{figure}


\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_sst_detail_nc_w8.pdf}
		\caption{8 worker-threads}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp41_sst_detail_nc_w64.pdf}
		\caption{64 worker-threads}
	\end{subfigure}%
	\caption{Server service time per server}\label{exp41_sst_detail_nc}
\end{figure}



\subsubsection{Explanation}

%Provide a detailed analysis of the results (e.g., bottleneck analysis, component utilizations, average queue lengths, system saturation). Add any additional figures and experiments that help you illustrate your point and support your claims.

\begin{itemize}
	\item bottleneck is the number of worker-threads (see component utilization)
	\item particular problem of write-only workload worker needs to wait for response from all servers and so the slowest server determines effect
	\item this waiting for slowest server is reason performance in middleware baseline cannot be replicated
	\item vm placement huge impact, here in experiments consistently server 2 slowest and hence automatically becomes part of the bottleneck
\end{itemize}

\subsection{Summary}

Based on the experiments above, fill out the following table with the data corresponding to the maximum throughput point for all four worker-thread scenarios.
\begin{itemize}
	\item wt=8 48 clients
	\item wt=16 72 clients
	\item wt=32 96 clients
	\item wt=64 144 clients
\end{itemize}

\begin{center}
	{Maximum throughput for the full system}
	\begin{tabular}{|l|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
		\hline                                            			& WT=8 & WT=16 & WT=32 & WT=64 \\ 
		\hline Throughput [ops/sec] (Middleware)                    & 6262 &  8102 & 10154 & 11206 \\ 
		\hline Throughput [ops/sec] (Derived from MW response time) & 6057 &  8081 & 10186 & 11562 \\ 
		\hline Throughput [ops/sec] (Client)              			& 6389 &  8250 & 10242 & 11391 \\ 
		\hline Average time in queue [ms]                 			&  4.0 &   3.6 &   2.1 &   1.3 \\ 
		\hline Average length of queue                    			&   12 &    15 &    11 &     8 \\ 
		\hline Average time waiting for memcached [ms]    			&  2.0 &   3.2 &   5.0 &   8.8 \\ 
		\hline 
	\end{tabular}
\end{center}

Based on the data provided in these tables, draw conclusions on the state of your system for a variable number of worker threads.

\section{Gets and Multi-gets (90 pts)}

For this set of experiments you will use three load generating machines, two middlewares and three memcached servers. Each memtier instance should have 2 virtual clients in total and the number of middleware worker threads is 64, or the one that provides the highest throughput in your system (whichever number of threads is smaller).

For multi-GET workloads, memtier will generate a mixture of SETs, GETs, and multi-GETs. Memtier only allows to specify the maximum number of keys in a multi-GET request. Therefore, be aware that requests can also contain fewer keys than the provided value. It is recommended to record the average size of the multi-GETs. You will have to measure response time on the client as a function of multi-get size, with and without sharding on the middlewares.
\todo{compare response time decomposition between sharded and non-sharded -> would expect sharded has longer wtt but shorter sst}

\subsection{Sharded Case}

Run multi-gets with 1, 3, 6 and 9 keys (memtier configuration) with sharding enabled (multi-gets are broken up into smaller multi-gets and spread across servers). Plot average response time as measured on the client, as well as the 25th, 50th, 75th, 90th and 99th percentiles.

\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 3                       \\ 
			\hline Number of client machines        & 3                       \\ 
			\hline Instances of memtier per machine & 2                       \\ 
			\hline Threads per memtier instance     & 1                       \\
			\hline Virtual clients per thread       & 2     		            \\ 
			\hline Workload                         & ratio=1:$<$Multi-Get size$>$             \\
			\hline Multi-Get behavior               & Sharded                 \\
			\hline Multi-Get size                   & [1..9]                  \\
			\hline Number of middlewares            & 2                       \\
			\hline Worker threads per middleware    & max. throughput config. \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)               \\ 
			\hline 
		\end{tabular}
	} 
\end{center}

\todo{better reference to experiment}
\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp51_sharded_rt_mget_perc_client.pdf}
		%\caption{}\label{fig1a}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		
	\end{subfigure}%
	\caption{Average response time and 25th, 50th, 75th, 90th and 99th percentiles of different multi-get sizes in sharded mode}
\end{figure}

\subsubsection{Explanation}

Provide a detailed analysis of the results (e.g., bottleneck analysis, component utilizations, average queue lengths, system saturation). Add any additional figures and experiments that help you illustrate your point and support your claims.

\subsection{Non-sharded Case}

Run multi-gets with 1, 3, 6 and 9 keys (memtier configuration) with sharding disabled. Plot average response time as measured on the client, as well as the 25th, 50th, 75th, 90th and 99th percentiles.

\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 3                       \\ 
			\hline Number of client machines        & 3                       \\ 
			\hline Instances of memtier per machine & 2                       \\ 
			\hline Threads per memtier instance     & 1                       \\
			\hline Virtual clients per thread       & 2                		 \\ 
			\hline Workload                         & ratio=1:$<$Multi-Get size$>$              \\
			\hline Multi-Get behavior               & Non-Sharded             \\
			\hline Multi-Get size                   & [1..9]                  \\
			\hline Number of middlewares            & 2                       \\
			\hline Worker threads per middleware    & max. throughput config. \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)               \\ 
			\hline 
		\end{tabular}
	} 
\end{center}

\todo{better reference to experiment}
\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp52_nonsharded_rt_mget_perc_client.pdf}
		%\caption{}\label{fig1a}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		
	\end{subfigure}%
	\caption{Average response time and 25th, 50th, 75th, 90th and 99th percentiles of different multi-get sizes in non-sharded mode}
\end{figure}

\subsubsection{Explanation}

Provide a detailed analysis of the results (e.g., bottleneck analysis, component utilizations, average queue lengths, system saturation). Add any additional figures and experiments that help you illustrate your point and support your claims.

\subsection{Histogram}

For the case with 6 keys inside the multi-get, display four histograms representing the sharded and non-sharded response time distribution, both as measured on the client, and inside the middleware. Choose the bucket size in the same way for all four, and such that there are at least 10 buckets on each of the graphs.

\todo{need better reference to experiment}
\begin{figure}
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp51_sharded_mget6_hist_mw.pdf}
		\caption{Sharded - Middleware}\label{fig1a}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp51_sharded_mget6_hist_client.pdf}
		\caption{Sharded - Client}\label{fig1b}
	\end{subfigure} \\
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp52_nonsharded_mget6_hist_mw.pdf}
		\caption{Non-Sharded - Middleware}\label{fig1a}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{data/exp52_nonsharded_mget6_hist_client.pdf}
		\caption{Non-Sharded - Client}\label{fig1b}
	\end{subfigure}%
	\caption{Sharded and non-sharded response time distirbution for multi-gets with 6 keys as measured inside the middleware and on the client}
\end{figure}

\subsection{Summary}

Provide a detailed comparison of the sharded and non-shareded modes. For which multi-GET size is sharding the preferred option? Provide a detailed analysis of your system. Add any additional figures and experiments that help you illustrate your point and support your claims.

\section{2K Analysis (90 pts)}

For 3 client machines (with 64 total virtual clients per client VM) measure the throughput and response time of your system in a 2k experiment with repetitions. All GET operations have a single key. Investigate the following parameters:

\begin{itemize}
		
	\item Memcached servers: 1 and 3
	\item Middlewares: 1 and 2
	\item Worker threads per MW: 8 and 32
	      	      
\end{itemize}

Repeat the experiment for (a)~a write-only and (b)~a read-only workload.
For each of the two workloads, what is the impact of these parameters on throughput, respectively response time?

\begin{center}
	\scriptsize{
		\begin{tabular}{|l|c|}
			\hline Number of servers                & 1 and 3                                     \\ 
			\hline Number of client machines        & 3                                           \\ 
			\hline Instances of memtier per machine & 1 (1 middleware) or 2 (2 middlewares) \\ 
			\hline Threads per memtier instance     & 2 (1 middleware) or 1 (2 middlewares)   \\
			\hline Virtual clients per thread       &  32                                     \\ 
			\hline Workload                         & Write-only and Read-only\\
			\hline Multi-Get behavior               & N/A                                         \\
			\hline Multi-Get size                   & N/A                                         \\
			\hline Number of middlewares            & 1 and 2                                     \\
			\hline Worker threads per middleware    & 8 and 32                                    \\
			\hline Repetitions                      & 3 or more (at least 1 minute each)                                   \\ 
			\hline 
		\end{tabular}
	} 
\end{center}


\begin{table}
	\centering
	\small{
		\caption{$2^k3$ Experiment Base Table for Write-Only}
		\label{exp60_wo_2k_base} 
		\setlength{\tabcolsep}{4.5pt}
		\input{data/exp60_wo_2k_base}
	}
\end{table}


\begin{table}
	\small{
		\centering
		\caption{Write-Only}
		\label{exp60_wo_2k_effect}
		\setlength{\tabcolsep}{4.7pt}
		\newcommand{\rlft}[0]{\raggedleft\arraybackslash}
		\input{data/exp60_wo_2k_effect}
	}
\end{table}



\begin{table}
	\centering
	\small{
		\caption{$2^k3$ Experiment Base Table for Read-Only}
		\label{exp60_ro_2k_base} 
		\setlength{\tabcolsep}{4.5pt}
		\input{data/exp60_ro_2k_base}
	}
\end{table}


\begin{table}
	\small{
		\centering
		\caption{Read-Only}
		\label{exp60_ro_2k_effect}
		\setlength{\tabcolsep}{4.7pt}
		\newcommand{\rlft}[0]{\raggedleft\arraybackslash}
		\input{data/exp60_ro_2k_effect}
	}
\end{table}

\section{Queuing Model (90 pts)}

Note that for queuing models it is enough to use the experimental results from the previous sections. It is, however, possible that the numbers you need are not only the ones in the figures we asked for, but also the internal measurements that you have obtained through instrumentation of your middleware.

\subsection{M/M/1}

Build queuing model based on Section 4 (write-only throughput) for each worker-thread configuration of the middleware. Use one M/M/1 queue to model your entire system. Motivate your choice of input parameters to the model. Explain for which experiments the predictions of the model match and for which they do not.

\subsection{M/M/m}

Build an M/M/m model based on Section 4, where each middleware worker thread is represented as one service.  Motivate your choice of input parameters to the model. Explain for which experiments the predictions of the model match and for which they do not.

\subsection{Network of Queues}

Based on Section 3, build a network of queues which simulates your system. Motivate the design of your network of queues and relate it wherever possible to a component of your system. Motivate your choice of input parameters for the different queues inside the network. Perform a detailed analysis of the utilization of each component and clearly state what the bottleneck of your system is. Explain for which experiments the predictions of the model match and for which they do not.


\bibliography{bibdb}{}
\bibliographystyle{plain}
\end{document}
